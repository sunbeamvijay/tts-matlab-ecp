\documentclass[12pt,a4paper]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage[dvips]{graphicx}
%\usepackage{times}

% Mathematical definition
\usepackage{amsthm}
\newtheorem{property}{Property}%[section]
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Proposition}%[section]
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}%[section]
\newtheorem{define}{Definition}%[section]
\newtheorem{definition}{Definition}%[section]

% Algorithm
\usepackage[ruled]{algorithm2e}

% Useful Mathematical command
\usepackage{amsfonts}% to get the \mathbb alphabet
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\C}{\field{C}}
\newcommand{\N}{\field{N}}
\newcommand{\K}{\field{K}}
\newcommand{\R}{\field{R}}
%\newcommand{\M}{\mathcal{M}}
\providecommand{\abs}[1]{ \left| #1  \right|}%{\lvert#1\rvert}
\providecommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\mathand}{\quad\text{ and }\quad}


\begin{document}

\tableofcontents

\section{Theorie}
\subsection{Entropie (en théorie de l'information)}

En théorie de l'information, l'entropie de Shannon est la mesure de l'incertitude associée avec une variable aléatoire. Elle quantifie l'information contenu dans un message habituellement en bits ou en bits par symboles. Elle correspond à la longueur minimale du message necessaire pour communiquer l'information. C'est donc la limite absolue pour la meilleur méthode de compression sans perte.

Pour une variable aléatoire discrète $X$, qui peut prendre les valeurs $\left\{ x_1,\dots,x_n \right\}$, cette entropie est égale à:
\[ H(X)  =  \operatorname{E}( I(X) ) = - \sum_{i=1}^np(x_i)\log_2 p(x_i) \]
%
où $I(X)$ est la quantité d'information de $X$ et $p(x_i)=Pr(X=x_i)$ est la probabilité d'occurence de $x_i$.

Avant de voir les propriétés de $H$, il est important de remarqué que ca definition dépend du modèle utilisé pour calculer la $p(x_i)$. En effet, un modele plus complexe (utilisant le contexte par exemple) permettra souvent de faire des predictions plus précise, et l'entropie $H$ diminura. C'est pourquoi, dire que $H$ ``correspond à la longueur minimale du message necessaire pour communiquer l'information'' est assez relatif et depend du modèle probabiliste.

%http://en.wikipedia.org/wiki/Information_entropy

L'entropie de Shannon est caractérisée par les propriétés suivantes.



\subsection{codage entropique}
=> huffman <=
arithmetic
\subsection{model}
=> static <=

/dynamic
avec contexe PPM

=> dictionnaire Lempel-Ziv-Welch (LZW) <=

DEFLATE is a lossless data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding => zipformat

\section{Fonctions Matlab}

sum
zeros

==, >, < vectorialise

plot, bar

sort

isstruct

\{ \} => cells

reshape

char

cree function et function intern

trouver lena plus petit

\section{TD}


\subsection{Compression sans perte}
Ecrire une fonction renvoyant la probabilite statique d'apparation d'un symbole dans un message. On prendra comme alphabet les nombres de 0 à 255, ce qui correspond à un octet.

Tracer le resultat pour l'image de lena et le texte

calculer l'entropie du message ordre 0, ordre 1

message: texte, image, xml/html\dots

donner la construction de l'arbre de huffman

% biblio
%http://en.wikipedia.org/wiki/Information_theory
%http://www.cs.cmu.edu/afs/cs/project/pscico-guyb/realworld/www/compression.pdf
%http://en.wikipedia.org/wiki/DEFLATE
%http://en.wikipedia.org/wiki/JPEG

\end{document}
