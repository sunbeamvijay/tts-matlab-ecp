\documentclass[12pt,a4paper]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{times}
\usepackage[latin1]{inputenc} % charactère accentué

% Algorithm
\usepackage[ruled]{algorithm2e}

% Useful Mathematical command
\usepackage{amsfonts}% to get the \mathbb alphabet
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\C}{\field{C}}
\newcommand{\N}{\field{N}}
\newcommand{\K}{\field{K}}
\newcommand{\R}{\field{R}}
%\newcommand{\M}{\mathcal{M}}
\providecommand{\abs}[1]{ \left| #1  \right|}%{\lvert#1\rvert}
\providecommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\mathand}{\quad\text{ and }\quad}


\title{TD TTS\\Compression sans perte}
\author{}

\usepackage[frenchb]{babel}

\begin{document}

\maketitle

\vfill
\tableofcontents
\vfill

\newpage

\section{Theorie}
\subsection{Entropie (en théorie de l'information)}

En théorie de l'information, l'entropie de Shannon est la mesure de l'incertitude associée avec une variable aléatoire. Elle quantifie l'information contenu dans un message habituellement en bits ou en bits par symboles. Elle correspond à la longueur minimale du message necessaire pour communiquer l'information. C'est donc la limite absolue pour la meilleur méthode de compression sans perte.

Pour une variable aléatoire discrète $X$, qui peut prendre les valeurs $\left\{ x_1,\dots,x_n \right\}$, cette entropie est égale à:
\[ H(X)  =  \operatorname{E}( I(X) ) = - \sum_{i=1}^np(x_i)\log_2 p(x_i) \]
%
où $I(X)$ est la quantité d'information de $X$ et $p(x_i)=Pr(X=x_i)$ est la probabilité d'occurence de $x_i$.

Avant de voir les propriétés de $H$, il est important de remarqué que ca definition dépend du modèle statistique utilisé pour calculer la $p(x_i)$. En effet, un modele plus complexe (utilisant le contexte par exemple) permettra souvent de faire des predictions plus précise, et l'entropie $H$ diminura. C'est pourquoi, dire que $H$ ``correspond à la longueur minimale du message necessaire pour communiquer l'information'' est assez relatif et depend du modèle probabiliste.

%http://en.wikipedia.org/wiki/Information_entropy

L'entropie de Shannon est caractérisée (définie à une constante près) par les propriétés suivantes:
(on notera $p_i=\Pr(X=x_i)$ et $H_n(p_1,\ldots,p_n)=H(X)$)

\paragraph{Continuité}
$H_n$ est continue (par rapport aux $p_i$).

\paragraph{Symetrique}
Changer l'ordre des probabilitée ne change pas la valeur de l'entropie

\paragraph{Maximum}
L'entropie doit être maximale quand toutes les possibilitées sont équiprobables.I.e.:
%
\[H_n(p_1,\ldots,p_n) \le H_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)\]
%
Pour des evenements equiprobables, l'entropie augmente strictement avec le nombre de possibilitées:
%
\[ H_n\bigg(\underbrace{\frac{1}{n}, \ldots, \frac{1}{n}}_{n}\bigg) < H_{n+1}\bigg(\underbrace{\frac{1}{n+1}, \ldots, \frac{1}{n+1}}_{n+1}\bigg) \]

\paragraph{Additivité}
Pour tout entiers strictement positifs $b_i$ avec $sum_{i=1}^{k}b_i=n$, on a:
%
\[ H_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = H_k\left(\frac{b_1}{n}, \ldots, \frac{b_k}{n}\right) + \sum_{i=1}^k \frac{b_i}{n} \, H_{b_i}\left(\frac{1}{b_i}, \ldots, \frac{1}{b_i}\right) \]

Cette derniere propriété permet de montrer que $H(1)=0$.

L'entropie d'un message est la somme des entropies de chaque symbole constituant le message.

\subsection{modèles statistiques}

L'objectif du modèle est d'estimer la probabilité d'apparence de chaque symbole à tel position dans le message. Autrement dit, le modèle essai de prédire le contenu du message à partir des informations qu'il a desja sur celui-ci.

Le modèle le plus simple, est un modèle static d'ordre 0. Dans se cas, la probabilité d'apparation d'un symbole (n'importe où dans le message) est égale au nombre d'occurences de ce symbole dans le message divisé par la longueur du message. Le premier probleme de ce modele est qu'il faut connaitre l'intégralité du message avant de pouvoir calculer les probabilités. Le deuxieme probleme, est qu'il ignore completement le contexe du symbole pour estimer sa probabilité, ce qui reduit grandement sont éfficacité.

Les modèles PPM d'ordre $n$ utilise les $n$ symboles precedent le symbole à predire pour estimer ca probabilité. C'est à dire que la probabilité que le symbole $x_{n+1}$ apparaise après les symboles $x_1\dots x_n$ est égale au nombre d'occurence de $x_1\dots x_nx_{n+1}$ dans le message diviser par le nombre d'occurence de $x_1\dots x_n$. Ces modèles utilise beaucoup plus de memoire que le précédent mais sont plus précis. Exemple, imaginons que l'on compresse un fichier texte en français, la probabilité qu'un 'u' suivent un 'q' est très forte de meme qu'un espace suit souvent un point ou une virgule. Un model d'ordre 1 donnera donc de bien meilleur resultat qu'un model d'ordre 0.


% The decoder must have the same model as the encoder.

\subsection{codage entropique}

Les codeurs entropiques sont des algorithmes qui essaient d'encoder un message avec le moins de bits possible en utilisant la probibilité de chaque symbole. La longeur minimal du message compressé est l'entropie de shannon. Les codeurs entropiques actuel sont très proche de cette limite. C'est pourquoi, pour ce genre de methode, tout la complexité est d'avoir le modèle statistique le plus precis possible afin de faire baisser l'entropie.

Un codeur entropique très connu, est l'algorithme de Huffman. Il consiste à creer un arbre binaire ayant pour feuilles les symboles à coder. Une fois l'arbre construit, on affect à chaque symbole un suite de bits. Cette suite est contruite en partant de la racine de l'arbre et en descendant jusqu'a la feuille du symbole en concatenant un 0 quand on prend le fils gauche et un 1 quand on descend par le fils droit (cf Figure \ref{fig:huffman}).
%
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[height=4cm]{huffman.png}
  \end{center}
  \caption{Arbre de huffman}
  \label{fig:huffman}
\end{figure}
%
Une des propriété de cette suite est qu'elle n'est le prefixe d'aucune autres suites parmis les symboles de l'arbre. Pour compression un message, on ecrit juste cette suite pour chaque symbole du message. Pour décoder, on lit les bits un à un et on descend l'arbre en partant de la racine. Si le bit est un 0 on descend à gauche, sinon à droit. Quand on arrive à une feuille on l'ajoute à la sortie et on repart de la racine de l'arbre.
Pour la construction de l'arbre, il y a un algorithme simple (algorithme \ref{alg:huff}). Chaque noeud de l'arbre contient en plus sa probabilité d'apparition. Pour une feuille, c'est la probabilité du symbole associé. Pour un noeud, c'est la somme des probabilitées de ses fils. L'algorithm utilise une liste de noeud.

\begin{algorithm}[htbm]
\SetLine
\dontprintsemicolon

   Initialiser la liste avec tout les symboles comme noeud (feuille) \;
  \While{la liste contient plus d'un element}{
    Retirer les deux noeuds ayant les plus faibles probabilitées de la liste\;
    Créer un nouveau noeud ayant pour fils les deux noeuds retirés\;
    Ajouter le nouveau noeud à la liste\;
  }
  Le seul noeud restant dans la liste est la racine de l'arbre\;

\caption{Construction de l'arbre de huffman}
\label{alg:huff}
\end{algorithm}

%http://en.wikipedia.org/wiki/Huffman_coding

Un codeur legerement plus performant est le codage arithmetique. Cependant, il est plus complexe et protégé par des brevets dans beaucoup de pays. Huffman est donc toujours utilisé dans de nombreuse application.
Un des problemes des modeles statistiques statique est qu'il faut envoyer les tables de frequence avec le message compressé pour que le decodeur puisse decompresser le message. Dans le cas de PPM, la taille de ces tables est loin d'etre négligeable.

\subsection{Modeles adaptatifs}

Les modèles 'adaptatif' ont la charactéristique de ne dependre que des symboles précédant celui à préduire, contrairement au modèles statiques qui doivent connaitre tout le message pour faire une prédiction. Cette notion est orthogonal avec celle de PPM. Un model PPM static doit connaitre tout le message. Ces modèles adaptatifs sont très utiles en compression car ils permettent de ne pas avoir à transmettre de trables statistiques.
La methode la plus courament utilisée va être décrite. A l'ensemble des symboles pouvant être présent dans le message, on ajoute un nouveau symbole d'échapement. Initialement, la probabilité de tout les symboles sauf celui d'echapement est nulle. A chaque fois qu'on compresse un nouveau symbole, on met à jour les tables de probabilitées pour prendre en compte ce symbole. Quand on rencontre un symbole à probabilité nulle, on utilise à la place le symbole d'echapement puis on inscrit le nouveau symbole non compresser avant de reprendre la conpression comme avant. Le decodeur peut construire ses tables de probabilitées au fur et a mesure de la decompression car elles ne dépendent que de symboles deja decompressés.


%http://en.wikipedia.org/wiki/PPM_compression_algorithm

%DEFLATE is a lossless data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding => zipformat

\subsection{Compression à dictionnaire}

Il existe une autre grande classe de méthodes de compression très utilisées. Ce sont les méthodes à dictionnaire. Parmis celles-ci, LZ77 utilisé dans le format zip et LZW une methode simple et rapide (utiliser par certains fichier tiff).

La méthode LZ77 (publié en 1977 par Lempel and Ziv) consiste à conserver les derniers symboles ecrits (2k à 32ko classiquement) et à remplacer quand c'est possible une serie de symboles par une réference à une serie de symboles précédente.

La methode Lempel-Ziv-Welch (LZW) utilise un dictionnaire. Un dictionnaire associe un code (un entier) à une chaine de charactère. Initialement, le dictionnaire est chargé avec toutes les chaines de un charactère (tout les symboles). Pour encoder, on cherche la plus longue chaine du dictionnaire qui correspond aux symboles à encoder et on écrit son code puis on ajoute au dictionnaire cette chaine avec le charactère suivant. L'algorithme \ref{alg:lzw-code} et l'algorithme \ref{alg:lzw-decode} décrive la procédure. \texttt{[a b]} représente la concaténation de a et b.
% revoir
Il y a un cas particulier auquel il faut faire attention pour la décompression. C'est le cas ou le compresseur rencontre cScSc, avec c un symbole et S une chaine de symboles quand cS est deja dans le dictionnaire.
%
Le decodeur sort le code pour cS  et ajoute cSc dans le dictionnaire. Ensuite, il voit cSc dans les symboles à encoder et il sort le nouveau symbole cSc. Or celui-ci n'est pas encore dans le dictionnaire du decodeur. Comme ce cas, est unique, le decodeur sait que s'il recoit un symbole qui n'est pas dans sont dictionaire, il doit considere que le charactere à ajouter est le premier de la chaine précedente.

\begin{algorithm}[htbm]
\SetLine
\dontprintsemicolon

  w = \texttt{''}\;
  \While{Il reste un symbole c en entrée}{
  \eIf{[w c] est dans le dictionnaire}{
    w = [w c]\;
    }{
    Inserer [w c] dans le dictionnaire\;
    ecrire en sortie le code associé à w\;
    w = c\;
    }
  }
  ecrire en sortie le code associé à w\;

\caption{Compression LZW}
\label{alg:lzw-code}
\end{algorithm}

\begin{algorithm}[htbm]
\SetLine
\dontprintsemicolon

   Lire un code k\;
   Ajouter k à la chaine de sortie (k est forcement un symbole)\;
   w = k\;
  \While{Il reste un code k en entrée}{
  \eIf{k$>$ nombre de symbole dans l'alphabet (256)}{
  \eIf{k existe dans le dictionnaire}{
    e = dictionnaire(k)\;
    }{
    \tcc{Le cas particulier discuté plus haut}
    e = [w w(0)]\;
    }
    }{
    \tcc{Ici, on peut directement convertir le code k en symbole}
    e = k\;
    }
    Ajouter e à la sortie\;
    Inserer [w e(0)] au dictionaire\;
    w = e\;
  }

\caption{Decompression LZW}
\label{alg:lzw-decode}
\end{algorithm}

%http://www.stanford.edu/class/ee398a/handouts/papers/WittenACM87ArithmCoding.pdf
%http://www.ics.uci.edu/~dan/pubs/DC-Sec4.html

%http://en.wikipedia.org/wiki/Golomb_coding

%http://en.wikipedia.org/wiki/Truncated_binary_encoding
%http://en.wikipedia.org/wiki/Unary_coding

\section{Fonctions Matlab}

isstruct, min, fopen, fread, fclose

\section{TD}

Ecrire une fonction renvoyant creant les tables statiques d'ordre 0 (c'est à dire la probabilite d'apparation de chaque symbole dans un message sans contexte). On prendra comme alphabet les nombres de 0 à 255, ce qui correspond à un octet. Utiliser la pour ecrire une fonction calculant l'entropie de shannon basée sur ce modele. Calculer l'entropie d'une phrase, d'une image, d'un texte\dots

\emph{L'entropie est en bit. Pour comparer avec la taille de l'entrée, il faut prendre en compte le facteur 8}

Recommencer avec un model PPM d'ordre 1.

Ecrire une fonction qui construit l'arbre de huffman pour le model d'ordre 0. Pour representer un noeud, il est conseiller d'utiliser un structure. Une structure est simplement une variable qui contient des champs. Il suffit de faire \texttt{a.b=1} pour creer une structure a avec un champ b ou ajouter un champ b a une structure existante. La fonction \texttt{min} peut renvoyer l'index du minimum.

Ecrire les fonctions de compression/decompression. Pour accelerer la compression, il est conseiller de calculer la chaine de bit associée avec chaque symbole une fois au debut et non pas à chaque fois qu'il faut encoder le symbole. Pour la decompression, on peut faire le meme genre de chose, mais c'est un peu plus compliqué.

Comparer la taille du flux compresser par huffman à l'entropie. Remarques.

Ecrire la compression/decompression LZW. Vous pourrez utiliser la fonction \texttt{create\_dico} fournie. Elle est très basique, mais suffisament rapide pour nos besoin. Voici un exemple d'utilisation:
\begin{verbatim}
dico = create_dico();
dico = dico.Insert(dico, 'ma chaine de charactère');
k=dico.Find(dico,'ma chaine de charactère');
if isempty(k)
  error('chaine non trouvé');
end
chaine = dico.Get(dico,k);
\end{verbatim}
Pour concatener deux chaines ou charactères, vous pouver simplement faire \texttt{[a, b]}.

Comparer le taux de compression avec celui de huffman. Conclusion.

\begin{thebibliography}{9}
  \bibitem{entropie} Entropie de Shanon, \\ \verb=http://en.wikipedia.org/wiki/Information_entropy=
  \bibitem{huff} Algorithm de Huffman, \\ \verb=http://en.wikipedia.org/wiki/Huffman_coding=
  \bibitem{ppm} Algorithme de compression PPM, \\ \verb=http://en.wikipedia.org/wiki/PPM_compression_algorithm=
  \bibitem{Arith} Codeur Arithmetique, \\ \verb=http://www.stanford.edu/class/ee398a/handouts/papers/=
              \verb=WittenACM87ArithmCoding.pdf=
  \bibitem{PSNR} LZW, \\ \verb=http://en.wikipedia.org/wiki/Lzw=
%  \bibitem{SSIM} SSIM, \\ \verb=http://www.cns.nyu.edu/~zwang/files/papers/ssim.html=
\end{thebibliography}


\end{document}
